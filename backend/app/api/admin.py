"""
Admin endpoints for database management and fixes.
Requires staff authentication.
"""
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from sqlalchemy import text
from app.database import get_db, engine
from app.properties.agent_assignment import (
    fix_all_agent_assignments,
    validate_agent_assignments,
    AGENT_PREFIX_MAP,
    ORPHAN_PREFIXES
)
from app.api.v1.auth import require_staff

router = APIRouter(prefix="/admin", tags=["admin"])


@router.post("/fix-all-agent-assignments")
def fix_all_agent_assignments_endpoint(
    db: Session = Depends(get_db),
    current_user: dict = Depends(require_staff)
):
    """
    Corrige agent_id de TODAS as propriedades baseado no prefixo da refer√™ncia.
    
    Utiliza o sistema autom√°tico de atribui√ß√£o que mapeia:
    - Prefixos de 2 letras: PR ‚Üí Paulo Rodrigues (37), AS ‚Üí Ant√≥nio Silva (24), etc.
    - Prefixos de 3 letras: JPE ‚Üí Jo√£o Pereira (33)
    - Prefixos √≥rf√£os: CB, FA, HA, JR, RC, SC ‚Üí Tiago Vindima (35, coordenador)
    
    Retorna estat√≠sticas detalhadas por agente.
    """
    results = fix_all_agent_assignments(db)
    return results


@router.get("/validate-agent-assignments")
def validate_agent_assignments_endpoint(
    db: Session = Depends(get_db),
    current_user: dict = Depends(require_staff)
):
    """
    Valida todas as atribui√ß√µes de agentes √†s propriedades.
    
    Retorna lista de propriedades com agent_id incorreto baseado no prefixo da refer√™ncia.
    √ötil para auditoria ap√≥s executar fix-all-agent-assignments.
    
    Resposta esperada ap√≥s corre√ß√£o: {"total": 336, "mismatches": []}
    """
    mismatches = validate_agent_assignments(db)
    return {
        "total": db.execute(text("SELECT COUNT(*) FROM properties")).scalar(),
        "mismatches": mismatches,
        "mismatches_count": len(mismatches),
        "status": "‚úÖ All correct" if len(mismatches) == 0 else f"‚ö†Ô∏è {len(mismatches)} mismatches found"
    }


@router.get("/agent-prefix-map")
def get_agent_prefix_map(current_user: dict = Depends(require_staff)):
    """
    Retorna o mapeamento completo de prefixos ‚Üí agent_id.
    
    √ötil para debugging e documenta√ß√£o.
    """
    return {
        "agent_prefix_map": AGENT_PREFIX_MAP,
        "orphan_prefixes": ORPHAN_PREFIXES,
        "total_agents": len(AGENT_PREFIX_MAP),
        "note": "Prefixes CB, FA, HA, JR, RC, SC are orphaned and assigned to coordinator (Tiago, ID 35)"
    }


@router.post("/migrate/leads")
def migrate_leads(current_user: dict = Depends(require_staff)):
    """
    üö® ENDPOINT TEMPOR√ÅRIO - Roda migra√ß√£o da tabela leads
    Adiciona colunas: source, origin, action_type, property_id
    """
    try:
        with engine.begin() as conn:
            results = []
            
            # Check and add source
            result = conn.execute(text("""
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name='leads' AND column_name='source'
            """))
            if result.fetchone() is None:
                conn.execute(text("ALTER TABLE leads ADD COLUMN source VARCHAR"))
                results.append("‚úÖ Added source")
            else:
                results.append("‚úì source exists")
            
            # Check and add origin
            result = conn.execute(text("""
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name='leads' AND column_name='origin'
            """))
            if result.fetchone() is None:
                conn.execute(text("ALTER TABLE leads ADD COLUMN origin VARCHAR"))
                results.append("‚úÖ Added origin")
            else:
                results.append("‚úì origin exists")
            
            # Check and add action_type
            result = conn.execute(text("""
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name='leads' AND column_name='action_type'
            """))
            if result.fetchone() is None:
                conn.execute(text("ALTER TABLE leads ADD COLUMN action_type VARCHAR"))
                results.append("‚úÖ Added action_type")
            else:
                results.append("‚úì action_type exists")
            
            # Check and add property_id
            result = conn.execute(text("""
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name='leads' AND column_name='property_id'
            """))
            if result.fetchone() is None:
                conn.execute(text("ALTER TABLE leads ADD COLUMN property_id INTEGER REFERENCES properties(id)"))
                results.append("‚úÖ Added property_id")
            else:
                results.append("‚úì property_id exists")
            
        return {
            "status": "success",
            "message": "Leads migration completed",
            "results": results
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/cleanup-old-media-urls")
def cleanup_old_media_urls(
    db: Session = Depends(get_db),
    current_user: dict = Depends(require_staff)
):
    """
    Remove URLs antigas de /media/ que retornam 404.
    
    ‚ö†Ô∏è Este endpoint √© tempor√°rio - usado ap√≥s migra√ß√£o para Cloudinary.
    Remove URLs antigas do Railway filesystem que j√° n√£o existem.
    Mant√©m URLs externas (Unsplash, Cloudinary, etc).
    """
    import json
    
    try:
        # Buscar propriedades com URLs antigas
        result = db.execute(text("""
            SELECT id, reference, images 
            FROM properties 
            WHERE images IS NOT NULL 
            AND images::text LIKE '%/media/properties/%'
        """))
        
        properties = result.fetchall()
        
        if not properties:
            return {
                "success": True,
                "message": "‚úÖ Nenhuma URL antiga encontrada!",
                "cleaned": 0,
                "properties": []
            }
        
        cleaned_list = []
        cleaned_count = 0
        
        for prop in properties:
            prop_id, reference, images_json = prop
            
            # Parse JSON array
            try:
                images = json.loads(images_json) if isinstance(images_json, str) else images_json
            except:
                images = images_json if isinstance(images_json, list) else []
            
            if not images:
                continue
            
            # Filtrar apenas URLs v√°lidas (externas)
            old_count = len(images)
            cleaned_images = [
                url for url in images 
                if not url.startswith('/media/') 
                and not 'railway.app/media/' in url
            ]
            
            removed = old_count - len(cleaned_images)
            
            if removed > 0:
                cleaned_list.append({
                    "id": prop_id,
                    "reference": reference,
                    "removed": removed,
                    "kept": len(cleaned_images)
                })
                
                # Update database
                if cleaned_images:
                    db.execute(text("""
                        UPDATE properties 
                        SET images = :images::jsonb
                        WHERE id = :id
                    """), {"images": json.dumps(cleaned_images), "id": prop_id})
                else:
                    # Se n√£o sobrou nenhuma, setar NULL
                    db.execute(text("""
                        UPDATE properties 
                        SET images = NULL
                        WHERE id = :id
                    """), {"id": prop_id})
                
                cleaned_count += 1
        
        db.commit()
        
        return {
            "success": True,
            "message": f"‚úÖ Limpeza conclu√≠da! {cleaned_count} propriedades atualizadas",
            "cleaned": cleaned_count,
            "total_found": len(properties),
            "properties": cleaned_list
        }
        
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Erro na limpeza: {str(e)}")


@router.get("/audit-database")
def audit_database(
    db: Session = Depends(get_db),
    current_user: dict = Depends(require_staff)
):
    """
    Gera relat√≥rio completo do estado da database.
    
    Retorna:
    - Estat√≠sticas de propriedades (total, com/sem imagens, com/sem v√≠deo)
    - Estat√≠sticas de agentes (total, com/sem foto, com/sem v√≠deo)
    - Breakdown por tipo de propriedade
    - Top agentes por n√∫mero de propriedades
    - An√°lise de URLs de imagens (Cloudinary vs Unsplash)
    """
    import json
    
    try:
        # === PROPRIEDADES ===
        total_props = db.execute(text("SELECT COUNT(*) FROM properties")).scalar()
        with_images = db.execute(text("""
            SELECT COUNT(*) FROM properties 
            WHERE images IS NOT NULL AND jsonb_array_length(images) > 0
        """)).scalar()
        without_images = total_props - with_images
        with_video = db.execute(text("""
            SELECT COUNT(*) FROM properties 
            WHERE video_url IS NOT NULL AND video_url != ''
        """)).scalar()
        published = db.execute(text("SELECT COUNT(*) FROM properties WHERE is_published = true")).scalar()
        
        # Breakdown por tipo
        tipos_result = db.execute(text("""
            SELECT property_type, COUNT(*) as count
            FROM properties
            GROUP BY property_type
            ORDER BY count DESC
        """)).fetchall()
        tipos = [{"tipo": t[0] or "N√£o definido", "count": t[1]} for t in tipos_result]
        
        # Top 10 agentes
        agentes_result = db.execute(text("""
            SELECT a.name, COUNT(p.id) as count
            FROM properties p
            LEFT JOIN agents a ON p.agent_id = a.id
            GROUP BY a.name
            ORDER BY count DESC
            LIMIT 10
        """)).fetchall()
        top_agentes = [{"name": a[0] or "Sem agente", "count": a[1]} for a in agentes_result]
        
        # === AGENTES ===
        total_agents = db.execute(text("SELECT COUNT(*) FROM agents")).scalar()
        agents_with_photo = db.execute(text("""
            SELECT COUNT(*) FROM agents 
            WHERE photo IS NOT NULL AND photo != ''
        """)).scalar()
        agents_without_photo = total_agents - agents_with_photo
        agents_with_video = db.execute(text("""
            SELECT COUNT(*) FROM agents 
            WHERE video_url IS NOT NULL AND video_url != ''
        """)).scalar()
        
        # === AN√ÅLISE DE IMAGENS ===
        images_result = db.execute(text("""
            SELECT 
                SUM(jsonb_array_length(images)) as total_images,
                COUNT(DISTINCT id) as properties_with_images
            FROM properties
            WHERE images IS NOT NULL
        """)).fetchone()
        
        total_images = images_result[0] or 0
        props_with_images = images_result[1] or 0
        avg_images = total_images / props_with_images if props_with_images > 0 else 0
        
        # URLs Unsplash vs Cloudinary
        unsplash_count = db.execute(text("""
            SELECT COUNT(DISTINCT id)
            FROM properties, jsonb_array_elements_text(images) as url
            WHERE url LIKE '%unsplash%'
        """)).scalar() or 0
        
        cloudinary_count = db.execute(text("""
            SELECT COUNT(DISTINCT id)
            FROM properties, jsonb_array_elements_text(images) as url
            WHERE url LIKE '%cloudinary%'
        """)).scalar() or 0
        
        # === V√çDEOS ===
        youtube_props = db.execute(text("""
            SELECT COUNT(*)
            FROM properties
            WHERE video_url LIKE '%youtube%' OR video_url LIKE '%youtu.be%'
        """)).scalar() or 0
        
        youtube_agents = db.execute(text("""
            SELECT COUNT(*)
            FROM agents
            WHERE video_url LIKE '%youtube%' OR video_url LIKE '%youtu.be%'
        """)).scalar() or 0
        
        return {
            "success": True,
            "properties": {
                "total": total_props,
                "published": published,
                "with_images": with_images,
                "without_images": without_images,
                "with_video": with_video,
                "without_video": total_props - with_video,
                "by_type": tipos,
                "top_agents": top_agentes
            },
            "agents": {
                "total": total_agents,
                "with_photo": agents_with_photo,
                "without_photo": agents_without_photo,
                "with_video": agents_with_video,
                "without_video": total_agents - agents_with_video
            },
            "images": {
                "total_urls": total_images,
                "avg_per_property": round(avg_images, 1),
                "unsplash_count": unsplash_count,
                "cloudinary_count": cloudinary_count
            },
            "videos": {
                "properties_youtube": youtube_props,
                "agents_youtube": youtube_agents
            },
            "priorities": {
                "critical": f"{without_images} propriedades SEM IMAGENS",
                "important": f"{agents_without_photo} agentes SEM FOTO",
                "optional": f"{total_props - with_video} propriedades SEM V√çDEO"
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erro ao auditar database: {str(e)}")

